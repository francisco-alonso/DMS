# ADR-002: Architecture of the Local Vision Processing Pipeline (Modular Design)

**Status:** Proposed  
**Date:** 2025-12-12  
**Author:** Francisco Lopez  

---

## 1. Context

ADR-000 defines the overall goal of building a Driver Monitoring System (DMS) capable of detecting driver drowsiness and distraction.

ADR-001 establishes MediaPipe FaceMesh as the landmark extraction technology for Phase 1.

This ADR defines the **local vision processing pipeline**, explaining how a single camera image (frame) is transformed step by step into meaningful numerical features and finally into a driver-state decision.

The architecture is intentionally **modular**, with clear components and data contracts, so it can be:
- easily understood by non–Computer Vision readers,
- implemented initially in a single process,
- evolved later toward edge deployment (Jetson) and cloud telemetry.

---

## 2. Fundamental Concepts (Clarification)

### 2.1 What is a Frame?

A **frame** is a single image captured by a camera at a specific moment in time.

- Example: a 640×480 RGB image.
- A camera produces many frames per second (e.g. ~30 FPS).
- Each frame represents one snapshot of reality.

A frame is **raw image data**.  
A frame is **not landmarks**.

---

### 2.2 What does MediaPipe produce?

MediaPipe analyzes a frame and:
- checks if a face is present,
- if a face is detected, outputs **468 predefined facial landmarks**.

So the relationship is:

- Input: one frame (image)
- Output: a list of 468 landmarks (only if a face is detected)

If no face is detected, no landmarks are produced.

---

### 2.3 What is a Landmark?

A **landmark** is a predefined anatomical point on a human face, such as:
- the corner of an eye,
- the edge of an eyelid,
- the tip of the nose,
- the corner of the mouth.

MediaPipe always uses the same 468 points, in the same order, for every face.

Each landmark is represented as:

- `x`: horizontal position in the image, normalized between 0.0 and 1.0  
- `y`: vertical position in the image, normalized between 0.0 and 1.0  
- `z`: relative depth value (not meters), used to estimate face orientation

Example:
- `x = 0.5` means the point is halfway across the image width.
- `y = 0.25` means the point is one quarter down the image height.

Landmarks are **derived geometric data**, not pixels.

---

## 3. Why Landmarks Are Converted into Features

Using 468 landmarks directly is impractical because they are:
- too numerous,
- noisy at the individual point level,
- difficult to interpret for decisions.

Therefore, landmarks are converted into **features**: compact numerical values that describe meaningful facial properties.

Examples of features:
- how open the eyes are,
- whether the mouth is open,
- whether the head is rotated.

---

## 4. Example Feature: Eye Aspect Ratio (EAR)

The **Eye Aspect Ratio (EAR)** is a numerical value that describes how open an eye is.

Conceptually:
- open eye → higher EAR value,
- closed eye → lower EAR value.

EAR is computed from a small subset of eye landmarks using simple geometry.

Typical values:
- eyes open: approximately 0.25–0.30
- normal blink: approximately 0.15–0.20
- eyes closed: below approximately 0.12

Example output for one frame:
- `ear_left = 0.21`
- `ear_right = 0.23`
- `ear_avg = 0.22`

These values:
- have no units,
- are ratios (not pixels or meters),
- summarize eye geometry in a way that is easy to reason about.

---

## 5. Proposed Pipeline Components and Contracts

The local vision pipeline is composed of the following components.  
Each component has a **clear responsibility** and **explicit input/output contract**.

---

### 5.1 FrameSource

**Responsibility:**  
Capture frames from the local webcam.

**Input:**  
- Camera device configuration (e.g. device index).

**Output (per frame):**
- `frame_id`: incremental integer.
- `timestamp_ms`: capture time in milliseconds.
- `image`: RGB image array (height × width × 3).

---

### 5.2 Preprocessor

**Responsibility:**  
Normalize frames to ensure stable and efficient downstream processing.

**Input:**  
- Frame from FrameSource.

**Operations (typical):**
- resize image (performance and consistency),
- color format conversion if required.

**Output:**
- Preprocessed image with the same semantic content, standardized format.
- Preserves `frame_id` and `timestamp_ms`.

---

### 5.3 LandmarkExtractor (MediaPipe)

**Responsibility:**  
Detect a face and extract facial landmarks.

**Input:**  
- Preprocessed frame.

**Output:**
- `frame_id`
- `timestamp_ms`
- `face_detected`: boolean
- If `face_detected = true`:
  - `landmarks`: list of 468 landmarks, each `(x, y, z)`

If no face is detected, no landmarks are produced and downstream processing for that frame is skipped.

---

### 5.4 FeatureExtractor

**Responsibility:**  
Convert landmarks into interpretable numerical features.

**Input:**  
- Landmark extraction result.

**Phase 1 Output Features:**
- `ear_left`
- `ear_right`
- `ear_avg`

**Future Features:**
- Mouth Aspect Ratio (MAR),
- head pose (yaw, pitch, roll),
- gaze direction.

**Output:**
- FeatureVector containing feature values and associated `frame_id` and `timestamp_ms`.

---

### 5.5 DecisionEngine

**Responsibility:**  
Interpret features **over time** to determine driver state.

**Input:**  
- FeatureVector from consecutive frames.

**Key principles:**
- Single-frame decisions are unreliable.
- Decisions use multiple consecutive frames.

**Example rule:**
- If `ear_avg` remains below a defined threshold for a defined number of consecutive frames, classify the state as drowsy.

**Output:**
- Driver state (e.g. ALERT, DROWSY).
- Associated timestamp.

---

### 5.6 EventSink

**Responsibility:**  
Persist or publish driver-state events.

**Input:**  
- Decision output.

**Phase 1 Output:**
- Local event logs (JSON or CSV).

**Future Output:**
- Telemetry events sent to a cloud backend.

---

## 6. Architectural Rationale

This modular architecture:
- separates concerns clearly,
- allows MediaPipe to be replaced later,
- supports migration to edge devices,
- supports future cloud integration,
- keeps Phase 1 simple while preserving long-term scalability.

---

## 7. Decision

We adopt a modular local vision processing pipeline where:
- frames are raw images,
- landmarks are derived geometric representations,
- features are compact numerical summaries,
- decisions are temporal and state-based.

This design balances clarity for learning with realism for future system evolution.

---

## 8. Next Steps

- Validate this ADR-002.
- Align the system diagram with these component contracts.
- Proceed to ADR-003: Temporal Decision Strategy.
- Begin Phase 1 implementation using this architecture.

---
