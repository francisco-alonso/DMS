# ADR-000: Project Purpose, Scope, and High-Level Vision for Driver Drowsiness & Distraction Monitoring System
- Status: Accepted
- Author: Francisco Lopez
- Date: 2025-12-05

## Context
Driver drowsiness and distraction are significant contributors to traffic accidents. We will build a multi-phase Driver Monitoring System (DMS) that processes camera frames to detect driver state (alert, distracted, drowsy) with real-time local inference and optional cloud telemetry, enabling hardware deployment on edge devices later. The project is intended as a learning and portfolio-grade engineering effort that demonstrates full-stack competence across computer vision, edge AI, IoT, and cloud-native microservices.

## Decision
We will implement a phased, iterative end-to-end system with strong documentation (ADRs for each technical decision). Phase 1 is a local, webcam-based MVP that detects eye closure and yawning using landmark-based features (e.g., EAR, MAR) with MediaPipe or equivalent. Later phases include edge deployment (Jetson/RPi), optimized model formats (ONNX/TensorRT), telemetry ingestion, time-series analytics, dashboards, fleet management, and MLOps.

## Goals
- Build a reproducible pipeline from camera → landmarks → features → decision → actuator/telemetry.
- Keep Phase 1 hardware-free for development
- Learn and document tactics for model selection, inference optimization, edge deployment, and distributed backends.
- Produce portfolio artifacts: working demo, readme, ADRs, architecture diagrams, dataset/metrics, and cloud-deployed services.

## Non-Goals (initial)
- We will not ship a certified automotive product.
- We will not attempt to collect sensitive personal data or stream full unencrypted video to cloud in early phases.
- We will not design full vehicle-integrated actuators until basic detection performance and safety are validated.

## Consequences
+ Positive:
  - Incremental learning and delivery.
  - Clear, industry-like artifacts for interviews (ADRs, diagram, demo).
  - Real edge deployment target to demonstrate optimization skills.
+ Negative / Trade-offs:
  - Long-term effort.
  - Potential for complexity creep; discipline needed to keep MVP scope bounded.

## Alternatives considered
1. Prototype-only (web demo): Fast but shallow; lacks hardware/edge credibility.
2. Cloud-only inference: Simple to implement but unsuitable for low-latency safety scenarios and privacy-sensitive data.
3. Full custom modeling from day one: High quality but extremely time-consuming (data collection + training).

Decision rationale: Phased approach blends speed-to-feedback (Phase 1) with realistic deployment targets (Phase 3+). Use well-supported libraries (MediaPipe) initially; consider custom or optimized models later based on measurement.

## Success criteria (Phase 1)
- Real-time face detection + landmark extraction on local Mac webcam.
- EAR/MAR metrics computed and validated empirically (save logs).
- Detect sustained eye closure > threshold with low false positive rate in controlled tests.
- Documented ADRs and architecture diagram for Phase 1.

## Next steps
1. Create end-to-end conceptual diagram (camera → preprocessing → detection → features → decision → actuator → telemetry → cloud).
2. ADR-001: Choose landmark extraction approach (MediaPipe vs dlib vs custom) and justify.
3. Implement Phase 1 prototype (local): code, tests, logging.
4. Collect controlled logs to define thresholds and metrics.
5. Plan Phase 2 (temporal models, edge/device selection, telemetry design).

## Monitoring & metrics
- Frame processing rate (FPS)
- Detection latency (ms per frame)
- False positive / false negative rates (for test scenarios)
- Event throughput (for future telemetry)

## Date
2025-12-06
